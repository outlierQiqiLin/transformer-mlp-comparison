# ResMLP 消融实验配置文件
# config/cifar10_resmlp_ablation.yaml

dataset:
  name: cifar10
  img_size: 224
  in_channels: 3
  num_classes: 10

# 所有变体使用相同的基础配置
models:
  # 变体1: 原始 ResMLP (baseline)
  resmlp_baseline:
    patch_size: 16
    dim: 384
    depth: 12
    layerscale_init: 0.1
    expansion_factor: 4
    dropout_rate: 0.0
    variant: baseline
  
  # 变体2: ResMLP + Attention
  resmlp_attn:
    patch_size: 16
    dim: 384
    depth: 12
    layerscale_init: 0.1
    expansion_factor: 4
    dropout_rate: 0.0
    num_heads: 8
    variant: attn
  
  # 变体3: ResMLP + LayerNorm (移除 Affine)
  resmlp_no_affine:
    patch_size: 16
    dim: 384
    depth: 12
    layerscale_init: 0.1
    expansion_factor: 4
    dropout_rate: 0.0
    variant: no_affine
  
  # 变体4: ResMLP 无 LayerScale
  resmlp_no_layerscale:
    patch_size: 16
    dim: 384
    depth: 12
    layerscale_init: 0.1  # 会被忽略
    expansion_factor: 4
    dropout_rate: 0.0
    variant: no_layerscale
  
  # 变体5: ResMLP 无 Cross-Patch (只有 MLP)
  resmlp_no_cross_patch:
    patch_size: 16
    dim: 384
    depth: 12
    layerscale_init: 0.1
    expansion_factor: 4
    dropout_rate: 0.0
    variant: no_cross_patch
  
  # 变体6: ResMLP Full (Attention + LayerNorm, 类似 ViT)
  resmlp_full:
    patch_size: 16
    dim: 384
    depth: 12
    layerscale_init: 0.1
    expansion_factor: 4
    dropout_rate: 0.0
    num_heads: 8
    variant: full

training:
  epochs: 20
  batch_size: 128
  num_workers: 1
  
  optimizer:
    name: adamw
    lr: 0.001
    weight_decay: 0.05
  
  scheduler:
    name: cosine
    min_lr: 0.000001

logging:
  checkpoint_dir: ./results/ablation_study
  save_freq: 50
  log_freq: 10