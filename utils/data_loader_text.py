# utils/data_loader_text.py
"""
ç”¨äºæ–‡æœ¬ä»»åŠ¡ (å¦‚ SST-2) çš„æ•°æ®åŠ è½½å™¨
"""
import torch
from torch.utils.data import DataLoader
from datasets import load_dataset
from transformers import AutoTokenizer

def get_dataloader_sst2(config):
    """ä¸º SST-2 åˆ›å»ºæ•°æ®åŠ è½½å™¨"""
    
    dataset_name = config.get('dataset.name')
    if dataset_name != 'sst2':
        raise ValueError("æ­¤åŠ è½½å™¨ä»…é€‚ç”¨äº SST-2")

    tokenizer_name = config.get('dataset.tokenizer_name')
    max_seq_len = config.get('dataset.max_seq_len')
    batch_size = config.get('training.batch_size')
    num_workers = config.get('training.num_workers', 4)

    # 1. åŠ è½½åˆ†è¯å™¨
    print(f"ğŸ“Š åŠ è½½åˆ†è¯å™¨: {tokenizer_name}")
    try:
        tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)
    except Exception as e:
        print(f"âŒ è‡ªåŠ¨ä¸‹è½½åˆ†è¯å™¨å¤±è´¥: {e}")
        raise
        
    vocab_size = tokenizer.vocab_size

    # 2. åŠ è½½æ•°æ®é›†
    print("ğŸ“Š åŠ è½½ SST-2 æ•°æ®é›†...")
    try:
        dataset = load_dataset('glue', 'sst2')
    except Exception as e:
        print(f"âŒ è‡ªåŠ¨ä¸‹è½½ SST-2 å¤±è´¥: {e}")
        raise

    # 3. å®šä¹‰é¢„å¤„ç†å‡½æ•°
    def preprocess_function(examples):
        # Hugging Face 'datasets' ä¼šè‡ªåŠ¨å¤„ç† 'sentence' åˆ—
        tokenized = tokenizer(
            examples['sentence'], 
            truncation=True, 
            padding='max_length', 
            max_length=max_seq_len
        )
        # å°† 'label' åˆ—é‡å‘½åä¸º 'labels' ä»¥ä¾¿ä¸ PyTorch å…¼å®¹
        tokenized['labels'] = examples['label']
        return tokenized

    print("ğŸ¤– é¢„å¤„ç†æ•°æ®é›†...")
    processed_dataset = dataset.map(
        preprocess_function, 
        batched=True,
        remove_columns=['sentence', 'idx', 'label'] # ç§»é™¤åŸå§‹åˆ—
    )

    # 4. è®¾ç½® PyTorch æ ¼å¼
    processed_dataset.set_format(
        type='torch', 
        columns=['input_ids', 'attention_mask', 'labels']
    )

    # 5. åˆ›å»º DataLoaders
    train_dataset = processed_dataset['train']
    test_dataset = processed_dataset['validation'] # SST-2 çš„æµ‹è¯•é›†æ²¡æœ‰æ ‡ç­¾ï¼Œæˆ‘ä»¬ç”¨éªŒè¯é›†

    train_loader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=num_workers,
        pin_memory=True
    )
    
    test_loader = DataLoader(
        test_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=num_workers,
        pin_memory=True
    )

    print(f"   è¯æ±‡è¡¨å¤§å° (Vocab Size): {vocab_size}")
    
    # è¿”å› vocab_sizeï¼Œæ¨¡å‹åˆ›å»ºæ—¶éœ€è¦
    return train_loader, test_loader, vocab_size