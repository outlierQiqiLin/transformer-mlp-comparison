# evaluate_efficiency.py
"""
è¯„ä¼°æ¨¡å‹æ•ˆç‡ (Latency å’Œ Throughput)

ç”¨æ³•:
    python evaluate_efficiency.py \
        --config ./results/ablation_study/resmlp_baseline_cifar10/config.yaml \
        --variant baseline \
        --checkpoint ./results/ablation_study/resmlp_baseline_cifar10/best_model.pth \
        --batch_size 64
"""

import torch
import time
import argparse
from tqdm import tqdm
import numpy as np

from utils.config import load_config
from models.ablation.resmlp_ablation import ResMLP_Ablation
from train_resmlp_ablation import create_ablation_model

def measure_efficiency(model, device, img_size, batch_size=64, warmup_steps=50, measure_steps=200):
    """
    æµ‹é‡æ¨¡å‹çš„ Latency å’Œ Throughput
    """
    model.eval()
    
    # 1. æµ‹é‡ Latency (å•æ ·æœ¬å»¶è¿Ÿ)
    print(f"\n--- 1. æµ‹é‡ Latency (Batch Size = 1) ---")
    dummy_input_latency = torch.randn(1, 3, img_size, img_size).to(device)
    
    # é¢„çƒ­
    print(f"é¢„çƒ­ (Warmup) {warmup_steps} æ­¥...")
    for _ in range(warmup_steps):
        _ = model(dummy_input_latency)
    
    if device.type == 'cuda':
        torch.cuda.synchronize()
        
    print(f"æµ‹é‡ {measure_steps} æ­¥...")
    timings = []
    with torch.no_grad():
        for _ in tqdm(range(measure_steps)):
            if device.type == 'cuda':
                torch.cuda.synchronize()
            start_time = time.perf_counter()
            
            _ = model(dummy_input_latency)
            
            if device.type == 'cuda':
                torch.cuda.synchronize()
            end_time = time.perf_counter()
            timings.append(end_time - start_time)
            
    avg_latency_s = np.mean(timings)
    avg_latency_ms = avg_latency_s * 1000
    
    print(f"âœ… å¹³å‡ Latency: {avg_latency_ms:.3f} ms")

    # 2. æµ‹é‡ Throughput (æŒ‡å®š Batch Size ååé‡)
    print(f"\n--- 2. æµ‹é‡ Throughput (Batch Size = {batch_size}) ---")
    dummy_input_throughput = torch.randn(batch_size, 3, img_size, img_size).to(device)
    
    # é¢„çƒ­
    print(f"é¢„çƒ­ (Warmup) {warmup_steps} æ­¥...")
    for _ in range(warmup_steps):
        _ = model(dummy_input_throughput)
        
    if device.type == 'cuda':
        torch.cuda.synchronize()
    
    print(f"æµ‹é‡ {measure_steps} æ­¥...")
    total_time = 0.0
    with torch.no_grad():
        if device.type == 'cuda':
            torch.cuda.synchronize()
        start_total_time = time.perf_counter()
        
        for _ in tqdm(range(measure_steps)):
            _ = model(dummy_input_throughput)
            
        if device.type == 'cuda':
            torch.cuda.synchronize()
        end_total_time = time.perf_counter()
        
    total_time = end_total_time - start_total_time
    total_samples = measure_steps * batch_size
    throughput_sps = total_samples / total_time # samples per second
    
    print(f"æ€»æ ·æœ¬æ•°: {total_samples}")
    print(f"æ€»æ—¶é—´: {total_time:.3f} s")
    print(f"âœ… å¹³å‡ Throughput: {throughput_sps:.2f} samples/sec")
    
    return avg_latency_ms, throughput_sps


def main():
    parser = argparse.ArgumentParser(description='ResMLP æ•ˆç‡è¯„ä¼°')
    parser.add_argument('--config', type=str, required=True,
                       help='è®­ç»ƒæ—¶ä½¿ç”¨çš„é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--variant', type=str, required=True,
                       choices=['baseline', 'attn', 'no_affine', 
                               'no_layerscale', 'no_cross_patch', 'full'],
                       help='æ¨¡å‹å˜ä½“')
    parser.add_argument('--checkpoint', type=str, required=True,
                       help='è®­ç»ƒå¥½çš„æ¨¡å‹æ£€æŸ¥ç‚¹è·¯å¾„ (e.g., best_model.pth)')
    parser.add_argument('--batch_size', type=int, default=64,
                       help='ç”¨äºæµ‹é‡ååé‡çš„æ‰¹é‡å¤§å°')
    args = parser.parse_args()

    # 1. åŠ è½½é…ç½®
    print(f"ğŸ“‚ åŠ è½½é…ç½®æ–‡ä»¶: {args.config}")
    config = load_config(args.config)
    
    # 2. è®¾ç½®è®¾å¤‡
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ğŸ–¥ï¸  ä½¿ç”¨è®¾å¤‡: {device}")
    
    # 3. åˆ›å»ºæ¨¡å‹
    model = create_ablation_model(config, args.variant, device)
    
    # 4. åŠ è½½æƒé‡
    print(f"ğŸ“¥ åŠ è½½æ¨¡å‹æƒé‡: {args.checkpoint}")
    try:
        checkpoint = torch.load(args.checkpoint, map_location=device)
        # å…¼å®¹æ€§å¤„ç†ï¼šæ£€æŸ¥ç‚¹å¯èƒ½ä¿å­˜äº† 'model_state_dict'
        if 'model_state_dict' in checkpoint:
            model.load_state_dict(checkpoint['model_state_dict'])
        else:
            model.load_state_dict(checkpoint)
        print("âœ… æƒé‡åŠ è½½æˆåŠŸ")
    except Exception as e:
        print(f"âŒ æƒé‡åŠ è½½å¤±è´¥: {e}")
        return

    # 5. è·å–å›¾åƒå¤§å°
    img_size = config.get('dataset.img_size')
    
    # 6. è¿è¡Œæ•ˆç‡æµ‹è¯•
    measure_efficiency(
        model, 
        device, 
        img_size, 
        batch_size=args.batch_size
    )

if __name__ == '__main__':
    main()


