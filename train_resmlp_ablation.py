# train_resmlp_ablation.py
"""
æ¶ˆèå®éªŒè®­ç»ƒè„šæœ¬
ç”¨æ³•:
    python train_resmlp_ablation.py --config config/cifar10_resmlp_ablation.yaml --variant baseline
    python train_resmlp_ablation.py --config config/cifar10_resmlp_ablation.yaml --variant attn
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from torchvision import datasets, transforms
import argparse
import os
from pathlib import Path
from tqdm import tqdm
import time

from utils.config import load_config, save_config
from models.ablation.resmlp_ablation import ResMLP_Ablation


def create_ablation_model(config, variant: str, device):
    """
    ä¸ºæ¶ˆèå®éªŒåˆ›å»ºæ¨¡å‹
    
    Args:
        config: é…ç½®å­—å…¸
        variant: æ¨¡å‹å˜ä½“åç§° (baseline, attn, no_affine, etc.)
        device: è®¾å¤‡
    """
    # æ„é€ æ¨¡å‹é…ç½®çš„ key
    model_key = f'resmlp_{variant}'
    model_config = config.get(f'models.{model_key}')
    
    if model_config is None:
        raise ValueError(f"é…ç½®æ–‡ä»¶ä¸­æ²¡æœ‰æ‰¾åˆ°æ¨¡å‹ '{model_key}' çš„é…ç½®")
    
    num_classes = config.get('dataset.num_classes')
    
    # åˆ›å»ºæ¶ˆèæ¨¡å‹
    model = ResMLP_Ablation(
        img_size=config.get('dataset.img_size'),
        patch_size=model_config['patch_size'],
        in_channels=config.get('dataset.in_channels', 3),
        dim=model_config['dim'],
        depth=model_config['depth'],
        layerscale_init=model_config.get('layerscale_init', 0.1),
        num_classes=num_classes,
        expansion_factor=model_config.get('expansion_factor', 4),
        dropout_rate=model_config.get('dropout_rate', 0.0),
        num_heads=model_config.get('num_heads', 8),
        variant=variant  # å…³é”®å‚æ•°ï¼šæŒ‡å®šå˜ä½“
    )
    
    return model.to(device)


def get_dataloader(config):
    """åˆ›å»ºæ•°æ®åŠ è½½å™¨ï¼ˆä¸åŸè®­ç»ƒè„šæœ¬ç›¸åŒï¼‰"""
    dataset_name = config.get('dataset.name')
    img_size = config.get('dataset.img_size')
    batch_size = config.get('training.batch_size')
    num_workers = config.get('training.num_workers', 4)
    
    if dataset_name == 'cifar10':
        mean = [0.4914, 0.4822, 0.4465]
        std = [0.2023, 0.1994, 0.2010]
        
        train_transform = transforms.Compose([
            transforms.Resize(img_size),
            transforms.RandomCrop(img_size, padding=4),
            transforms.RandomHorizontalFlip(),
            transforms.ToTensor(),
            transforms.Normalize(mean, std)
        ])
        
        test_transform = transforms.Compose([
            transforms.Resize(img_size),
            transforms.ToTensor(),
            transforms.Normalize(mean, std)
        ])
        
        try:
            train_dataset = datasets.CIFAR10(
                root='./data', 
                train=True, 
                download=True, 
                transform=train_transform
            )
            
            test_dataset = datasets.CIFAR10(
                root='./data', 
                train=False, 
                download=True, 
                transform=test_transform
            )
        except Exception as e:
            print(f"\nâŒ è‡ªåŠ¨ä¸‹è½½å¤±è´¥: {str(e)}")
            print("è¯·å…ˆæ‰‹åŠ¨ä¸‹è½½æ•°æ®é›†")
            raise
        
    elif dataset_name == 'cifar100':
        mean = [0.5071, 0.4867, 0.4408]
        std = [0.2675, 0.2565, 0.2761]
        
        train_transform = transforms.Compose([
            transforms.Resize(img_size),
            transforms.RandomCrop(img_size, padding=4),
            transforms.RandomHorizontalFlip(),
            transforms.ToTensor(),
            transforms.Normalize(mean, std)
        ])
        
        test_transform = transforms.Compose([
            transforms.Resize(img_size),
            transforms.ToTensor(),
            transforms.Normalize(mean, std)
        ])
        
        try:
            train_dataset = datasets.CIFAR100(
                root='./data', 
                train=True, 
                download=True, 
                transform=train_transform
            )
            
            test_dataset = datasets.CIFAR100(
                root='./data', 
                train=False, 
                download=True, 
                transform=test_transform
            )
        except Exception as e:
            print(f"\nâŒ è‡ªåŠ¨ä¸‹è½½å¤±è´¥")
            raise
    
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„æ•°æ®é›†: {dataset_name}")
    
    train_loader = DataLoader(
        train_dataset, 
        batch_size=batch_size, 
        shuffle=True, 
        num_workers=num_workers,
        pin_memory=True
    )
    
    test_loader = DataLoader(
        test_dataset, 
        batch_size=batch_size, 
        shuffle=False, 
        num_workers=num_workers,
        pin_memory=True
    )
    
    return train_loader, test_loader


def create_optimizer(model, config):
    """åˆ›å»ºä¼˜åŒ–å™¨"""
    optimizer_config = config.get('training.optimizer')
    optimizer_name = optimizer_config['name']
    lr = float(optimizer_config['lr'])
    weight_decay = float(optimizer_config.get('weight_decay', 0))
    
    if optimizer_name == 'adam':
        optimizer = optim.Adam(
            model.parameters(), 
            lr=lr, 
            weight_decay=weight_decay
        )
    elif optimizer_name == 'adamw':
        optimizer = optim.AdamW(
            model.parameters(), 
            lr=lr, 
            weight_decay=weight_decay
        )
    elif optimizer_name == 'sgd':
        momentum = optimizer_config.get('momentum', 0.9)
        optimizer = optim.SGD(
            model.parameters(), 
            lr=lr, 
            momentum=momentum, 
            weight_decay=weight_decay
        )
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„ä¼˜åŒ–å™¨: {optimizer_name}")
    
    return optimizer


def create_scheduler(optimizer, config):
    """åˆ›å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨"""
    scheduler_config = config.get('training.scheduler')
    
    if scheduler_config is None:
        return None
    
    scheduler_name = scheduler_config['name']
    
    if scheduler_name == 'cosine':
        epochs = config.get('training.epochs')
        min_lr = scheduler_config.get('min_lr', 0)
        if isinstance(min_lr, str):
            min_lr = float(min_lr)
        scheduler = optim.lr_scheduler.CosineAnnealingLR(
            optimizer, 
            T_max=epochs,
            eta_min=min_lr
        )
    elif scheduler_name == 'step':
        step_size = scheduler_config.get('step_size', 30)
        gamma = scheduler_config.get('gamma', 0.1)
        scheduler = optim.lr_scheduler.StepLR(
            optimizer, 
            step_size=step_size, 
            gamma=gamma
        )
    elif scheduler_name == 'multistep':
        milestones = scheduler_config.get('milestones', [60, 120, 160])
        gamma = scheduler_config.get('gamma', 0.2)
        scheduler = optim.lr_scheduler.MultiStepLR(
            optimizer, 
            milestones=milestones, 
            gamma=gamma
        )
    else:
        return None
    
    return scheduler


def train_epoch(model, train_loader, criterion, optimizer, device, epoch):
    """è®­ç»ƒä¸€ä¸ªepoch"""
    model.train()
    running_loss = 0.0
    correct = 0
    total = 0
    
    pbar = tqdm(train_loader, desc=f'Epoch {epoch}')
    for batch_idx, (inputs, targets) in enumerate(pbar):
        inputs, targets = inputs.to(device), targets.to(device)
        
        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, targets)
        
        loss.backward()
        optimizer.step()
        
        running_loss += loss.item()
        _, predicted = outputs.max(1)
        total += targets.size(0)
        correct += predicted.eq(targets).sum().item()
        
        pbar.set_postfix({
            'loss': running_loss / (batch_idx + 1),
            'acc': 100. * correct / total
        })
    
    epoch_loss = running_loss / len(train_loader)
    epoch_acc = 100. * correct / total
    
    return epoch_loss, epoch_acc


def validate(model, test_loader, criterion, device):
    """éªŒè¯æ¨¡å‹"""
    model.eval()
    test_loss = 0
    correct = 0
    total = 0
    
    with torch.no_grad():
        for inputs, targets in tqdm(test_loader, desc='Validating'):
            inputs, targets = inputs.to(device), targets.to(device)
            outputs = model(inputs)
            loss = criterion(outputs, targets)
            
            test_loss += loss.item()
            _, predicted = outputs.max(1)
            total += targets.size(0)
            correct += predicted.eq(targets).sum().item()
    
    avg_loss = test_loss / len(test_loader)
    accuracy = 100. * correct / total
    
    return avg_loss, accuracy


def save_checkpoint(model, optimizer, epoch, best_acc, save_path):
    """ä¿å­˜æ£€æŸ¥ç‚¹"""
    checkpoint = {
        'epoch': epoch,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'best_acc': best_acc,
    }
    torch.save(checkpoint, save_path)


def train(model, train_loader, test_loader, config, save_dir, device):
    """å®Œæ•´çš„è®­ç»ƒæµç¨‹"""
    epochs = config.get('training.epochs')
    
    criterion = nn.CrossEntropyLoss()
    optimizer = create_optimizer(model, config)
    scheduler = create_scheduler(optimizer, config)
    
    best_acc = 0.0
    train_losses = []
    train_accs = []
    test_losses = []
    test_accs = []
    
    print(f"\n{'='*60}")
    print(f"å¼€å§‹è®­ç»ƒ...")
    print(f"{'='*60}\n")
    
    for epoch in range(1, epochs + 1):
        start_time = time.time()
        
        train_loss, train_acc = train_epoch(
            model, train_loader, criterion, optimizer, device, epoch
        )
        
        test_loss, test_acc = validate(model, test_loader, criterion, device)
        
        if scheduler is not None:
            scheduler.step()
            current_lr = scheduler.get_last_lr()[0]
        else:
            current_lr = optimizer.param_groups[0]['lr']
        
        train_losses.append(train_loss)
        train_accs.append(train_acc)
        test_losses.append(test_loss)
        test_accs.append(test_acc)
        
        epoch_time = time.time() - start_time
        
        print(f'\nEpoch: {epoch}/{epochs}')
        print(f'Time: {epoch_time:.2f}s | LR: {current_lr:.6f}')
        print(f'Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%')
        print(f'Test Loss: {test_loss:.4f} | Test Acc: {test_acc:.2f}%')
        
        if test_acc > best_acc:
            print(f'âœ… éªŒè¯å‡†ç¡®ç‡æå‡: {best_acc:.2f}% -> {test_acc:.2f}%')
            best_acc = test_acc
            save_checkpoint(
                model, optimizer, epoch, best_acc,
                os.path.join(save_dir, 'best_model.pth')
            )
        
        if epoch % config.get('logging.save_freq', 50) == 0:
            save_checkpoint(
                model, optimizer, epoch, best_acc,
                os.path.join(save_dir, f'checkpoint_epoch_{epoch}.pth')
            )
        
        print('-' * 60)
    
    save_checkpoint(
        model, optimizer, epochs, best_acc,
        os.path.join(save_dir, 'final_model.pth')
    )
    
    print(f"\n{'='*60}")
    print(f"è®­ç»ƒå®Œæˆ!")
    print(f"æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_acc:.2f}%")
    print(f"æ¨¡å‹ä¿å­˜è·¯å¾„: {save_dir}")
    print(f"{'='*60}\n")
    
    return {
        'train_losses': train_losses,
        'train_accs': train_accs,
        'test_losses': test_losses,
        'test_accs': test_accs,
        'best_acc': best_acc
    }


def main():
    parser = argparse.ArgumentParser(description='ResMLP æ¶ˆèå®éªŒè®­ç»ƒ')
    parser.add_argument('--config', type=str, required=True,
                       help='é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--variant', type=str, required=True,
                       choices=['baseline', 'attn', 'no_affine', 
                               'no_layerscale', 'no_cross_patch', 'full'],
                       help='æ¨¡å‹å˜ä½“')
    parser.add_argument('--resume', type=str, default=None,
                       help='ä»æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒ')
    args = parser.parse_args()
    
    # 1. åŠ è½½é…ç½®æ–‡ä»¶
    print(f"ğŸ“‚ åŠ è½½é…ç½®æ–‡ä»¶: {args.config}")
    config = load_config(args.config)
    
    # 2. æ˜¾ç¤ºå…³é”®é…ç½®
    variant_names = {
        'baseline': 'ResMLP (Baseline)',
        'attn': 'ResMLP + Attention',
        'no_affine': 'ResMLP + LayerNorm',
        'no_layerscale': 'ResMLP - LayerScale',
        'no_cross_patch': 'ResMLP - CrossPatch',
        'full': 'ResMLP Full (ç±» ViT)'
    }
    
    print("\n" + "="*60)
    print("æ¶ˆèå®éªŒé…ç½®:")
    print(f"  å˜ä½“: {variant_names[args.variant]}")
    print(f"  æ•°æ®é›†: {config.get('dataset.name')}")
    print(f"  æ‰¹é‡å¤§å°: {config.get('training.batch_size')}")
    print(f"  è®­ç»ƒè½®æ•°: {config.get('training.epochs')}")
    print(f"  å­¦ä¹ ç‡: {config.get('training.optimizer.lr')}")
    print(f"  ä¼˜åŒ–å™¨: {config.get('training.optimizer.name')}")
    if config.get('training.scheduler'):
        print(f"  è°ƒåº¦å™¨: {config.get('training.scheduler.name')}")
    print("="*60 + "\n")
    
    # 3. è®¾ç½®è®¾å¤‡
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ğŸ–¥ï¸  ä½¿ç”¨è®¾å¤‡: {device}")
    if torch.cuda.is_available():
        print(f"   GPU: {torch.cuda.get_device_name(0)}")
    print()
    
    # 4. åˆ›å»ºä¿å­˜ç›®å½•
    exp_name = f"resmlp_{args.variant}_{config.get('dataset.name')}"
    save_dir = os.path.join(config.get('logging.checkpoint_dir'), exp_name)
    Path(save_dir).mkdir(parents=True, exist_ok=True)
    
    # 5. ä¿å­˜é…ç½®
    save_config(config, os.path.join(save_dir, 'config.yaml'))
    print(f"âœ… é…ç½®å·²ä¿å­˜åˆ°: {os.path.join(save_dir, 'config.yaml')}")
    
    # 6. åˆ›å»ºæ¨¡å‹
    model = create_ablation_model(config, args.variant, device)
    
    param_count = model.count_parameters()
    
    print(f"âœ… æ¨¡å‹åˆ›å»ºæˆåŠŸ")
    print(f"   å˜ä½“: {args.variant}")
    print(f"   å‚æ•°é‡: {param_count:,}")
    print(f"   ä¿å­˜ç›®å½•: {save_dir}\n")
    
    # 7. åŠ è½½æ•°æ®
    print("ğŸ“Š åŠ è½½æ•°æ®é›†...")
    train_loader, test_loader = get_dataloader(config)
    print(f"   è®­ç»ƒæ ·æœ¬: {len(train_loader.dataset)}")
    print(f"   æµ‹è¯•æ ·æœ¬: {len(test_loader.dataset)}")
    print(f"   æ‰¹æ¬¡æ•°: {len(train_loader)} (train), {len(test_loader)} (test)\n")
    
    # 8. ä»æ£€æŸ¥ç‚¹æ¢å¤
    if args.resume:
        print(f"ğŸ“¥ ä»æ£€æŸ¥ç‚¹æ¢å¤: {args.resume}")
        checkpoint = torch.load(args.resume, map_location=device)
        model.load_state_dict(checkpoint['model_state_dict'])
        print(f"   ä» epoch {checkpoint['epoch']} ç»§ç»­è®­ç»ƒ\n")
    
    # 9. å¼€å§‹è®­ç»ƒ
    results = train(model, train_loader, test_loader, config, save_dir, device)
    
    # 10. ä¿å­˜è®­ç»ƒç»“æœ
    torch.save(results, os.path.join(save_dir, 'training_results.pth'))
    print(f"ğŸ’¾ è®­ç»ƒç»“æœå·²ä¿å­˜åˆ°: {os.path.join(save_dir, 'training_results.pth')}")


if __name__ == '__main__':
    main()